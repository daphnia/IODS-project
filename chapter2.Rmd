# RStudio Exercise 2: Regression and Model Validation

<!-- > *INSTRUCTIONS:*
> *Describe the work you have done this week and summarize your learning.*
> 
> - Describe your work and results clearly. 
> - Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods
> - Assume the reader has no previous knowledge of your data or the more advanced methods you are using  -->

## Data Wrangling and General Remarks
First I did some data wrangling, i.e. formatting a dataset to be suitable and nice to be used in an analysis later, in this case in regression analysis. The data wrangling was done in a R Script file *create_learning2014.R* and the formatted dataset is in a file *learning2014.csv*. As described in the wrangling file the data was originally collected for international survey of Approaches to Learning. For details and links to further information have a look at the [create_learning2014.R](https://github.com/daphnia/IODS-project/blob/master/data/create_learning2014.R) at my [repository](https://github.com/daphnia/IODS-project).

After practising and doing a bit of the exercise the RStudio workspace became a bit cluttered. So, I found that a function `remove(...)` can be used to remove objects from the memory. It is handy to clean up the environment once in a while.

Another important thing is that you have to load the extra libraries every time you start RStudio, even though files create_learning2014.R, .RData and .Rhistory are used to load the situation where you were when quitting last time.

##Analysis

Let's first read the data set into memory and have a look at the data set. Summary off the variables are below:
```{r echo=T}
#getwd()
#NOTE: some functions are commented out to make the knitted output more simple.
learning2014 <- read.csv("data/learning2014.csv", header = TRUE)
str(learning2014)
summary(learning2014)
```
It consists of 166 observations of 7 variables. The variables deep (deep learning), stra (strategic learning) and surf (surface learning) represents three different dimensions of the way of learning. They have been calculated as means of several other variables in the previous data wrangling. This data is in Likert scale and based on data originally obtained from a questionnaire.

There is a noticeable deviation (*skewness*) in age at the highest quartile, shifting the mean age away from median value towards the older end of the distribution. This can be seen clearly from a histogram below: 
```{r echo=T}
library(ggplot2)
p_age <- ggplot(learning2014, aes(age)) + geom_histogram(binwidth = 1, fill="purple", colour="red") + ggtitle("Histogram of Student's Ages")
p_age
```

Scatter plots of all the variables using gender as a factor are presented in a matrix below. This is just to show, how much better plots can be produced by GGally and ggplot2 libraries. Those are found below this first picture: 
```{r echo=T, fig.cap="**In this scatter plot matrix black dots marks females and red dots marks males**", fig.height=9, fig.width=12}
pairs(learning2014[-1], col=learning2014$gender)
```

GGally and ggplot2 produces another, more informative, scatterplot matrix:
```{r echo=T, fig.cap="**In this scatter plot matrix red color marks females and blue color marks males**",fig.height=9, fig.width=12}
library(ggplot2)
library(GGally)
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```

* It can be seen from the matrix above that there are remarkably more females than males.
* It could be analysed if there is statistically significant correlation between gender and attitude (note: there is a tail or a tiny peak of males with low attitude).
* Gender probably does not explain statistically if one has obtained surface learning as a strategy, although the distributions look a bit different between genders because of *kurtosis*. It means that only the variances are different between genders when means are about the same. The variability is higher among males. 
* Also correlation between attitude vs. exam points should be checked. Alternative hypothesis could be: higher attitude results in higher points. 
* There could be a statistically significant negative correlation between deep and surf, which is quite obvious according to common sense. In correlation coefficients there is also some difference between genders, which probably is explained by a single female outlier. **According to literature, the correlation coefficient is very sensitive for outliers**, i.e.. abnormal/unexpected values. 
* Interesting is if there is not statistically significant correlation between deep learning and exam points, especially among females (note, there might be a female outlier which could explain the low correlation coefficient). This relationship needs more research. 
* I cannot see any non-linear relationships. **NOTE:** it is always important to look both the scatter plot and correlation coefficient as even though the correlation coefficient is low there might be **non-linear correlation** seen from scatter plot. Also the low correlation coefficient can be a result of the **sensitivity for outliers**. 

###Linear regression model
For the following regression analysis the exam *points* will be a target (aka dependent, response) variable and let's choose three independent (aka explanatory) variables: 

1. *attitude* towards data science
2. use of *deep* learning strategies
3. use of *surf*ace learning strategies.

Here is a regression model with multiple explanatory variables:
```{r echo=T}
my_model <- lm(points ~ attitude + surf + deep, data = learning2014)
summary(my_model)
```
From the summary above it can be seen that only attitude has statistically significant relationship with exam points (p=1.18e-08 so the risk that this is a wrong conclusion, is extremely low). Standard error (0.05766) of regression coefficient for attitude is not an order of magnitude less than the coefficient itself (0.34661), indicating that there is some variability in the estimate for the coefficient. R-squared is about 20 %. It means that on the average only 20 % of variability in predicted exam points can be explained by these three variables. The F-statistic indicates, would the model be better with fewer variables. In that case the p-value would be higher. Now p-value is 5.217e-08. Removing *surf* from the model gives a bit lower p-value 2.326e-08. Removing also the *deep* gives even lower p-value 4.119e-09. 

After testing also the other variables and checking the p-values from F-tests, it can be concluded that the best model has only the attitude as an explanatory variable. According to this model below, a student has to rise attitude by three numbers in order to increase exam points by one number. Also the intercept suggests that even if a student has zero attitude, the exam points would be 12 (from the simplified model below), which is quite reasonable.

```{r echo=T}
my_best_model <- lm(points ~ attitude, data = learning2014)
summary(my_best_model)
```

###Model validation

Let's make some graphical model validation using the following plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage.



```{r echo=T, fig.cap="**A model *my_model* explaining exam points with attitude towards data science, use of deep learning strategies and use of surface learning strategies**"}
par(mfrow = c(2,2))
plot(my_model, which = c(1,2,5))
```

```{r echo=T, fig.cap="**A model *my_best_model* explaining exam points with attitude towards data science**"}
par(mfrow = c(2,2))
plot(my_best_model, which = c(1,2,5))
```

As the model is linear regression it is assumed that the relationship between attitude and exam points are linear. From the scatter plots further above it can be seen that this is not exactly the case: there are some observations where exam points are exceptionally low regardless of the attitude. It can be imagined, that these observations reflects some problems – not attitude problems – in gathering exam points. 

1. Residuals vs fitted values is a plot that can be used to check the model assumption that the residuals (ε, size of the errors) do not depend on explanatory variables but only on random variability (in other words, residuals has a constant variance ($σ^{2}$). In the plot the dots should be randomly distributed, otherwise it indicates a problem.
    i) In these cases there can be seen some tails and also a group of outliers near the x-axle. 
2. Normal QQ-plot can be used to check, are residuals distributed normally. If they are, the dots should be aligned on the line.
    i) In these cases there can be seen some tails.
3. Residuals vs leverage is a plot that can be used to assess how big effect a single observation has to the model. In the plot a big x value means exceptionally big effect and refers to that the observation in question might be an outlier (*Note*: if an exceptional value (y axle) of target variable is near the mean of explanatory variables of the model, the observation does not have so big (leverage) effect to the regression line).
    i) In the first model there is clearly a single exceptional value.